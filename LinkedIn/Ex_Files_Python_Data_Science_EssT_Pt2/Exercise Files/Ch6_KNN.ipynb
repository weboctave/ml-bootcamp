{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Other Popular Machine Learning Models Models\n",
    "## Segment 3 - Instance-based learning w/ k-Nearest Neighbor\n",
    "#### Setting up for classification analysis\n",
    "https://www.linkedin.com/learning/python-for-data-science-essential-training-part-2/instance-based-learning-with-knn?u=36492188"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import urllib\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True) \n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 7, 4\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = './Data/mtcars.csv'\n",
    "\n",
    "cars = pd.read_csv(address)\n",
    "cars.columns = ['car_names','mpg','cyl','disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']\n",
    "\n",
    "X_prime = cars[['mpg', 'disp', 'hp', 'wt']].values\n",
    "y = cars.iloc[:,9].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_names</th>\n",
       "      <th>mpg</th>\n",
       "      <th>cyl</th>\n",
       "      <th>disp</th>\n",
       "      <th>hp</th>\n",
       "      <th>drat</th>\n",
       "      <th>wt</th>\n",
       "      <th>qsec</th>\n",
       "      <th>vs</th>\n",
       "      <th>am</th>\n",
       "      <th>gear</th>\n",
       "      <th>carb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mazda RX4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.620</td>\n",
       "      <td>16.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mazda RX4 Wag</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>160.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.875</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Datsun 710</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>93</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.320</td>\n",
       "      <td>18.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hornet 4 Drive</td>\n",
       "      <td>21.4</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110</td>\n",
       "      <td>3.08</td>\n",
       "      <td>3.215</td>\n",
       "      <td>19.44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hornet Sportabout</td>\n",
       "      <td>18.7</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.440</td>\n",
       "      <td>17.02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           car_names   mpg  cyl   disp   hp  drat     wt   qsec  vs  am  gear  \\\n",
       "0          Mazda RX4  21.0    6  160.0  110  3.90  2.620  16.46   0   1     4   \n",
       "1      Mazda RX4 Wag  21.0    6  160.0  110  3.90  2.875  17.02   0   1     4   \n",
       "2         Datsun 710  22.8    4  108.0   93  3.85  2.320  18.61   1   1     4   \n",
       "3     Hornet 4 Drive  21.4    6  258.0  110  3.08  3.215  19.44   1   0     3   \n",
       "4  Hornet Sportabout  18.7    8  360.0  175  3.15  3.440  17.02   0   0     3   \n",
       "\n",
       "   carb  \n",
       "0     4  \n",
       "1     4  \n",
       "2     1  \n",
       "3     1  \n",
       "4     2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 21.   , 160.   , 110.   ,   2.62 ],\n",
       "       [ 21.   , 160.   , 110.   ,   2.875],\n",
       "       [ 22.8  , 108.   ,  93.   ,   2.32 ],\n",
       "       [ 21.4  , 258.   , 110.   ,   3.215],\n",
       "       [ 18.7  , 360.   , 175.   ,   3.44 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prime[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we can run KNN, we need to scale our variables\n",
    "# it is one of the model assumption.  always satisfy the model assumption\n",
    "X = preprocessing.scale(X_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(32, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1533, -0.5798, -0.5437, -0.6202],\n",
       "       [ 0.1533, -0.5798, -0.5437, -0.3554],\n",
       "       [ 0.4567, -1.006 , -0.7956, -0.9317],\n",
       "       [ 0.2207,  0.2236, -0.5437, -0.0023],\n",
       "       [-0.2344,  1.0598,  0.4195,  0.2313]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.shape)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=.2, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and training your model with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n"
     ]
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating your model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         4\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.86         7\n",
      "   macro avg       0.90      0.83      0.84         7\n",
      "weighted avg       0.89      0.86      0.85         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred= clf.predict(X_test)\n",
    "y_expect = y_test\n",
    "\n",
    "print(metrics.classification_report(y_expect, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall - a measure of model's completeness. \n",
    "    Of all points that were labled 1, only 67% of results that were returned were truly relevant.\n",
    "    Of the entire dataset, 83% of results that were returned were truly relevant.\n",
    "High Precision + Low Recall = Few results returned, but many of the label predictions that are returned are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance-based learning with KNN\n",
    "Selecting transcript lines in this section will navigate to timestamp in the video\n",
    "- [Instructor] K-Nearest Neighbor Classification is a supervised machine learning method that you can use to classify instances based on arithmetic difference between features in a labeled dataset. In the coding demonstration for this segment, you're going to see how to predict whether a car has an automatic or a manual transmission based on its number of gears and carburetors. K-Nearest Neighbor works by memorizing observations within a labeled test set to predict classification labels for new incoming unlabeled observations. The algorithm makes predictions based on how similar training observations are to the new incoming observations. The more similar the observation's values, the more likely they will be classified with the same label. Popular use cases for K-Nearest Neighbor include stock price prediction, credit risk analysis, predictive trip planning, and recommendation systems. We need to look at the K-Nearest Neighbor assumptions. So the K-Nearest Neighbor model assumes that your dataset has little noise, that, of course, it's labeled, that your dataset contains only relevant features, and that the dataset has distinguishable subgroups. You also want to avoid using K-Nearest Neighbor on large datasets because it's probably going to take a very, very long time depending on how big your dataset actually is. So let's use Python to implement the K-Nearest Neighbor algorithm. So here we are in the Jupyter Notebook, and it's coming preloaded with most of the libraries that you need. I just want to show you how to access the K-Nearest Neighbor Classifier. So it's in scikit-learn. So we'll say from sklearn. And it's in the neighbors module. We'll say .neighbors. We need to import KNeighborsClassifier, and run this, okay. And your Jupyter Notebook came preloaded with the settings that you need for printing out your data visualizations, so you don't need to worry about that, just run it. And it also came with the data already preloaded in terms of like the column names and stuff like that. Remember that you do need to go and change your file pathway to map to your mtcars dataset. But aside from that, you should be good to go. So let's start by just creating some DataFrames with this. So let's start by just creating a DataFrame called X_prime. And we will set that equal to our cars dataset, but then we only want to isolate out a small subset of that dataset, so we're going to be working with mpg variable, which is miles per gallon. We're going to work with displacement, disp. We want to import hp, or horsepower. And then, also weight. Okay, and we only want the values here, so we are going to say .values. I need to add square bracket here. Okay, let's also set a value for our target. So our target, of course, is going to be the automatic and manual transmission, so that's the am variable. So we will say y is equal to cars.lo, so we're using the ILO indexer in order to select that column. And we want to select column at index position nine, and we only want the value, so we're going to say .values. Okay, and then we're going to run this. Before we can implement the K-Nearest Neighbor algorithm, we need to scale our variables. So as usual I'm working through our model assumptions and just making sure we prepare our data properly, and that it actually qualifies. It actually satisfies the assumptions of our model. It's important to always do that. So let's use the scale function from preprocessing module of scikit-learn. To do that, we're going to say preprocessing.scale, and we'll pass in our X_prime data. So let's call the output of this X, so this is going to be our X data, and we'll run it. That scales our variables. Now I'm going to split the data into test and training sets. We use training set for training the model, and test set for evaluating the model's performance. So to do this, we are going to first just call the train test split function, train_test_split, and then we'll pass in our data. So that's going to be X and y, and then let's make it 20% of our overall dataset size. So we'll say test is equal to test_size is equal to .2. And in this case, in this demo we'll set a random state equal to 17. And that's just so you get the same results on your machine as I'm getting on mine. Now, we need to name our variables here. So what this function's going to output is it's going to output our X training data, we'll call that X_train. Our X test data, and we'll call that X_test. Our y train data, and our y test data. Okay, so I'm going to run this. Now it's time to build our model. The first thing we'll need to do is instantiate a classifier, we'll call it clf, and we're going to set that equal to neighbors.KNeighborsClassifier, that's the function in scikit-learn, and then we're going to call the fit function and pass in our data. So that's going to be clf.fit, pass in X_train and y_train since we're training our model. And then let's just print out our model so we can get an idea of the default model parameter. So print clf. So these are our default model parameters, and now let's just go ahead and evaluate the model's predictions. So we'll generate a test point and we'll call it y_pred. So we're going to say y_pred is equal to, and then we will call the predict function. So we're going to say clf.predict, pass in our X_test data. And then our expected values are actually our y test data, so we'll just say y_expect equals to y_test, just to make it a little more human-readable. Now let's go ahead and use the classification report function in order to evaluate how well the model performed. So to do that, we need to access the metrics module from within sklearn. And within that, the classification report function. Okay, and then we're going to pass the data that we want it to use for the evaluation, so that's going to be y_expect and y_pred. And then we want to print this whole thing out, so I'm going to call the print function on this whole thing, okay, and then run it. Okay, so it's okay. I mean, there's a decent performance here. All right, so here we are. These are our results from the demonstration we just did. And I just want to explain what these mean real quick. So recall is a measure of a model's completeness. And what these results are saying is that of all the points that were labeled one, only 67% of those results that were returned were truly relevant. And of the entire dataset, 83% of the results that were returned were truly relevant. High precision and low recall generally means that there are fewer results returned, but many of the labels that are predicted are returned correctly. In other words, high accuracy but low completion. That's it for instance-based learning. Hold on, because in the next demonstration, I'm going to show you how to use Python for decision trees with CART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
